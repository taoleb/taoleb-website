---
title: "从李飞飞的研究看25年空间智能发展"
categories: "AI技术"
tags: ['空间智能', '李飞飞', 'AI', '斯坦福', '2025']
id: li-feifei-spatial-intelligence-2025
date: '2025-11-07'
pubDate: '2025-11-07'
updatedDate: '2025-11-07'
cover: "https://wp-cdn.4ce.cn/v2/qIo0X8z.jpeg"
recommend: true
top: false
hide: false
---

:::note
2025年，人工智能正从“感知世界”跃迁至“理解并重建世界”。以斯坦福大学李飞飞教授团队的研究为坐标，我们可以清晰地看到一条通向“空间智能”的演进路径。本文将解析其团队在2025年的关键成果，从认知基础、生成重建到物理具身三个维度，探讨空间智能的未来图景。
:::

### 引言：超越2D，AI的下一站是空间智能
在过去，人工智能的巨大突破主要体现在对二维图像、文本等平面数据的理解上。然而，我们所处的物理世界是三维的、动态的。要实现通用人工智能（AGI），AI不仅要“看懂”，更要能“理解”三维空间，乃至在其中“行动”。

“空间智能”（Spatial Intelligence）——即对三维空间及其动态关系的建模、推理、生成与交互能力，正成为定义下一代AI能力的核心战场。

2025年，斯坦福大学HAI研究院李飞飞教授团队的一系列研究，系统性地展示了从视觉理解走向具身智能的演进。本文将深入解读这些工作，勾勒出空间智能发展的三大关键脉络。

### 脉络一：认知基石——AI如何理解与想象空间？
空间智能的基础，是AI能否像人类一样，建立一个关于空间的“心智模型”（Mental Model）。这不仅关乎静态场景的布局，更关乎动态时序中的关系推理。

#### 1. 补全“想象”：从有限视角构建空间心智模型 (Spatial Mental Modeling)

人类可以轻易地从几个有限的视角“脑补”出整个房间的布局。但对AI而言，这是一个巨大挑战。李飞飞团队为此提出了 MINDCUBE 基准，专门测试VLM（视觉语言模型）在视角转换、遮挡推理和“假设性移动”时的空间理解力。

研究发现，现有VLM几乎无法完成这一任务。为此，团队提出“先生成认知地图，再进行推理” (map-then-reason) 的框架，通过让模型先构建一个内在的空间结构表示，再回答问题，使得准确率飙升了30%以上。这揭示了，让AI“在脑中画地图”是实现类人空间想象的第一步。

#### 2. 增加“时间”：从静态空间走向时空推理 (Fine-Grained Video QA)

真实世界是四维的（三维空间 + 一维时间）。团队的 MOMA-QA 数据集与 SGVLM 模型，推动视频问答（VideoQA）进入“细粒度”时代。

这项工作不再满足于“视频里有什么”，而是追问“在什么时间、什么空间、实体间发生了什么关系”。通过引入场景图预测和帧级检索，SGVLM模型得以实现更高层次的时空推理与多主体认知。

#### 3. 精准“重建”：从2D线索到3D人体 (NeuHMR)

空间智能也体现在对特定对象的精确三维理解上。在人体运动重建（HMR）领域，传统方法严重依赖可能出错的2D关键点。

NeuHMR 框架则另辟蹊径，引入神经渲染（HuNeRF）技术。它不再依赖2D伪标注，而是通过“渲染的图像是否与原始视频一致”这一低层视觉约束，反向优化三维人体网格的姿态与形状。这种“眼见为实”的优化方式，显著提升了复杂动态场景下人体重建的鲁棒性。

### 脉络二：生成重建——AI如何从2D知识“创造”3D世界？
如果说理解是输入，那么生成就是输出。空间智能的另一大支柱，是利用已有的庞大2D知识库（如互联网图像），来“生成”和“重建”三维世界，以解决3D数据稀缺的难题。

#### 1. 跨越“维度”：用2D扩散模型生成3D高斯 (Gaussian Atlas)

2D的扩散模型（如Stable Diffusion）已炉火纯青，但3D生成的效果却受限于高质量3D数据不足。Gaussian Atlas 框架提出了一种天才的“降维打击”思路。

它创新地将三维高斯（3D Gaussian Splatting）通过特定数学变换，“展平”为二维的稠密网格（Atlas）。这样一来，强大的2D扩散模型就可以直接在“展平”的二维流形上学习三维结构特征。该研究同时构建了大规模 GaussianVerse 数据集，成功搭建起2D与3D生成建模的桥梁。

#### 2. 优化“引擎”：高效探索扩散模型架构 (Grafting)

要支撑3D这样更复杂、更高数据通量的生成任务，底层的生成引擎（如Diffusion Transformer, DiT）必须更高效。

Grafting（嫁接）技术提供了一种全新的、低成本的架构探索范式。它不是从零开始训练，而是在已预训练的DiT模型上，直接“嫁接”新的结构模块（如替换注意力机制、改变MLP比例）。实验证明，这种方法仅需2%的预训练算力，就能达到接近原模型的生成质量，并实现显著的推理加速。这是实现高效3D生成的技术基座。

### 脉络三：物理具身——AI如何“进入”并“行动”于空间？
空间智能的最终闭环，是让智能体（Agent）进入物理世界，与环境交互。这要求AI不仅理解空间，更能理解空间中的“可供性”（Affordance）并执行复杂的“全身运动”。

#### 1. 学习“交互”：无监督蒸馏可供性知识 (UAD)

“可供性”回答了“这个物体能用来做什么？”（例如，杯子是“可抓握”的，门把手是“可转动”的）。UAD (Unsupervised Affordance Distillation) 框架解决了如何让机器人“无师自通”地学习可供性。

UAD利用大型VLM的理解能力，自动从无标注数据中提取<指令, 可供性>的配对数据，并“蒸馏”到一个轻量级模型中。这使得机器人仅需极少量（约10条）演示，就能将操作泛化到未见过的物体和任务，为零标注、开放世界的机器人操控提供了新路径。

#### 2. 实现“行动”：从数据采集到全身操控 (BEHAVIOR ROBOT SUITE & MOMAGEN)

真实世界的家庭任务（如打扫、烹饪）极其复杂，需要机器人具备双臂协同、精确导航和全身可达性。

BEHAVIOR ROBOT SUITE (BRS) 提供了一个完整的“硬件+软件”研究平台。它基于具备双臂和四自由度躯干的移动平台，集成了低成本的远程操作系统，为机器人在真实家庭环境中学习全身操控提供了基准。

MOMAGEN 则解决了训练数据的“鸡生蛋”问题。在多步双臂操作中，采集高质量演示数据极难。MOMAGEN将此过程变为一个约束优化问题（平衡可达性、可见性等），能从单条人类演示中，自动化、大规模地生成多样化、高质量的机器人演示数据，极大提升了模仿学习的效率和泛化能力。

### 总结：迈向真正“活在”物理世界的AI
李飞飞团队2025年的研究图谱，为我们勾勒出一条“空间智能”的清晰演进路径：

- **认知层**： AI正通过“心智建模”（Spatial Mental Modeling）和“时空推理”（MOMA-QA），从看懂像素进化到理解空间关系。
- **生成层**： AI正利用“跨维迁移”（Gaussian Atlas）和“架构优化”（Grafting），将海量的2D知识转化为重建3D世界的能力。
- **行动层**： AI正通过“可供性学习”（UAD）和“全身操控”（BRS & MOMAGEN），将理解和生成的能力，最终闭环于物理世界的真实交互。

这三大脉络共同指向一个未来：人工智能不再是局限于屏幕的旁观者，而是将成为能理解、重建并最终“活在”我们物理空间中的智能体。
